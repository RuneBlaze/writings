Title         : Welcome
Author        : You
Logo          : True

[TITLE]

## 18.A.

We prove the following proposition.

~ Proposition
For any deterministic algorithm, for any integer $h \geq 0$, for a tree (that is a valid rooted tree described in the problem)
of height $h$, there exists an adversary (filling out the leaf labels) that forces the algorithm to read all $n = 5^h$ leaves.
In addition, this adversarial algorithm satisfies the following property: even if the algorithm knows $n - 1$
leaf labels, the adversary can force the algorithm to output any output of our desire (0 or 1) by choosing the $n$-th
leaf label at our pleasure (I think this is implied by the assumption, but it never hurts to additionally assume it).
Let this adversary be denoted as $A(h)$.
~

~ Proof
We prove by strong induction on $h$. We start with our strong induction base cases.
Consider the base cases of $h = 0, 1$.
We can obviously construct $A(0)$: the algorithm must evaluate the only leaf, and
the adversary can assign the label at our pleasure to force the algorithm to
output any output. When $h = 1$,
the adversary first returns 0 upon the first leaf query, then 1, then 0, then 1, which
forces the algorithm to query the fifth leaf. For the fifth leaf query, the adversary labels
the leaf to be either $1$ (if we want the algorithm to output 1) or $0$ (if
we want the algorithm to output 0). Thus
we have now invented an adversary $A(1)$. We are done with
the base cases.

Now we do the inductive case.
Let $h > 1$, consider the five children of the root, all of which has height $h - 1$.
Initialize five $A(h-1)$ adversaries and assign each of them to the five children
in any order (we have these adversaries by induction). For getting the value of the root,
the algorithm must first evaluate three of the children (of the root). Because we have
valid adversaries for the children (that have height $h - 1$), the adversaries will force
the algorithm to evaluate $3 \cdot 5^{h-1}$ leaves. Moreover, we can force arbitrary
evaluation results of the three children no matter how the algorithm queries the leaf node
labels (we can always change the evaluation results "last minute" due to properties
of the adversaries). We force the results to be 0, 0, 1. The algorithm must evaluate
a fourth child, and by a similar argument this child can be forced to have evaluation 1.
The fifth child also needs to have all its leaves evaluated -- $5^{h-1}$ leaves in total,
and this fifth child can be forced to have arbitrary evaluation by the induction assumption.
Moreover, if the algorithm only evaluated $5^{h}-1$ nodes, this implies that for one immediate
subtree of the root, it has not been evaluated and we can force any evaluation of that subtree's root.
The other four subtrees and their roots can be forced to have evaluations 0, 0, 1, 1, and we can force
any evaluation at our wish by forcing the evaluation of the subtree that has not been evaluated yet.
Therefore we have proven our inductive case.
~

The above proposition implies that for arbitrary $h$ we can constuct an adversary as desired.

## 18.B.

We define the value of a non-leaf node to be the value returned by that node. Given a node $u$, if the value
of that node is defined, we denote $v_u$ to be the value of that node. We first prove a lemma.

~ Lemma
Under the assumptions, fix $u$ as an arbitrary node, then $v_u$, considered as a random variable, takes value 1 and value 0
with equal probability; that is, $\mathbb{P}[v_u = 1] = 0.5$.
~

~ Proof
We proceed by structural induction on the tree. If $u$ is a leaf node, then by implicit assumption (hopefully, or else I don't know the distribution
of the leaf node values and I have no way to properly calculate the constant $c$) $u$ satisfies our lemma. We now do structural induction,
consider nodes $u_1, \ldots, u_5$ s.t. $w$ is the parent of all such $u_i$, with $u_i$ satisfying the lemma (i.e. they take value 1
and value 0 with equal probabiltiy). Now we can calculate $\mathbb{P}[v_w = 1]$, which is equal to (the denominator is counting
the total number of values all $u_i$ can take. The numerator is counting the total configurations of $u_i$ where 3 of them is 1; 4 of
them is 1; 5 of them is 1).

$$
\frac{\binom{5}{3} + \binom{5}{4} + \binom{5}{5}}{2^5} = 0.5
$$

Then $\mathbb{P}[v_w = 0] = 0.5$, which satisfies our lemma. Our induction goal is proven. Thus the lemma is proven.
~

Let $f(h)$ denote the expected number of leaves evaluated by the algorithm considered (dependent on $h$, obviously). If $h = 0$, $f(h)$ takes
the obviously value of $1$ (we have to look at that leaf/root!). Consider the case where $h > 0$, obviously, we have to in random order
evaluate at least three children of the root (by definition of the algorithm), where each evaluation takes expectedly $f(h-1)$ leaf evaluations
by property of expectation. In more details, we have the following cases:

 1. We (in random order. We always evaluate in random order in the children (i.e. shuffle the children and evaluate in order), this statement will be omitted in the following bullet points)
 evaluate three children of the root. Very fortunately they all agree. We are done.
 2. We evaluate the first three children and they disagree. The fourth one leads to an agreement. We are done.
 3. We evaluate the first three -- they disagree. The fourth one still does not lead to an agreement. Evaluate the final one. We are done.

The first case means that we have evaluated three children. Each children takes expectedly $f(h-1)$ leaf evaluations to be evaluated -- in total
$3 f(h-1)$ leaf evaluations expectedly. Similarly the second case means we have $4 f(h-1)$ leaf evaluations expectedly. Similarly
the third case means that we have expectedly $5 f(h-1)$ leaf evaluations. Now since the value of any node is essentially a coin flip,
we can now calculate the probability of the cases.

 1. The first case. The denominator is $2^3$ since we evaluated three values. Each value is essentially a fair coin, and we want them
 to be either all tails or all heads. There are two cases: all tails and all heads. The numerator is $2$.
 2. The second case. The denominator is $2^4$ by similar logic. Consider the evaluations in order (after essentially shuffling) of the children,
 the values can be one of the following six: 0111, 1011, 1101 (consensus -- 1), 1000, 0100, 0010 (consensus -- 0). Therefore the numerator is $6$.
 3. The third case. The denominator is $2^5$. To count for the numerator. The first four values obtained from the evaluations must have two 1s and two 0s,
 and the final value can either be 1 or 0 (depending on the consensus value). Therefore the numerator is $\binom{4}{2}$ (choosing where to put the 1s in
 the first four positions) times $2$, i.e. $\binom{4}{2} 2$.

Therefore we have the following recurrence for $f(h)$:

 - if $h = 0$, $f(h) = 1$
 - otherwise, $f(h) = 3 f(h-1) \frac{2}{2^3} + 4 f(h-1) \frac{6}{2^4} + 5 f (h - 1) \frac{2\binom{4}{2}}{2^5}$

Simplify the general case: we have $f(h) = \frac{33}{8}f(h-1)$. Therefore the closed form solution for $f(h)$ is $\frac{33}{8}^h$.
Therefore for the input, we expectedly read $\frac{33}{8}^h$ leaves of the tree. We now transform this value:

$$
\frac{33}{8}^h = \frac{33}{8}^{\lg_5 n} = n^{\lg_5 \frac{33}{8}} \approx n^{0.88} < n^{0.9}
$$

Therefore obviously the expected number of leaves evaluated is at most $O(n^{0.9})$.


[reference manual]: http://research.microsoft.com/en-us/um/people/daan/madoko/doc/reference.html  "Madoko reference manual"
